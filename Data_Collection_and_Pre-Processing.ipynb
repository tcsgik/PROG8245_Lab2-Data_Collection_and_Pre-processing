{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed3eb9a5",
   "metadata": {},
   "source": [
    "# Lab2 - Data Collection and Pre-processing\n",
    "\n",
    "## Data Requirements\n",
    "1. Purpose: Primary transactions file\n",
    "    -  Source: Any public e-commerce sample with ≥ 500 rows (e.g. the “1000 Sales Records” CSV on ExcelBIAnalytics¹ → keep first 500) OR the 500-row synthetic file created in class\n",
    "    - Notes: Must contain: date, customer_id, product, price, quantity, coupon_code (or promo field), shipping_city\n",
    "    - Reference source: https://excelbianalytics.com/wp/downloads-18-sample-csv-files-data-sets-for-testing-sales/\n",
    "    - Data source: Using some item of Reference source to generate primary_dataset.csv\n",
    "2. Purpose: Secondary metadata file\n",
    "    - Source: A second open data source of your choice (product catalogue, city look-ups, coupon descriptions, etc.)\n",
    "    - Notes: You will mine this file to build your Data Dictionary and (optionally) enhance features\n",
    "    - Data source: Using some item of Reference source to generate secondary_dataset.csv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b89274",
   "metadata": {},
   "source": [
    "## How to Generate coupon code in primary_dataset.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "5bcd1756",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv('Original_Dataset/primary_dataset.csv')\n",
    "\n",
    "import random\n",
    "import string\n",
    "\n",
    "# Generate coupon code\n",
    "def generate_coupon_code():\n",
    "    return ''.join(random.choices(string.ascii_uppercase + string.digits, k=8))\n",
    "\n",
    "# Assign a coupon code to about 30% of rows, leave the rest as empty\n",
    "df['Coupon Code'] = [generate_coupon_code() if random.random() < 0.3 else '' for _ in range(len(df))]\n",
    "\n",
    "# Save new CSV file with the coupon codes\n",
    "df.to_csv('Original_Dataset/primary_dataset_with_CouponCode.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8542d4b5",
   "metadata": {},
   "source": [
    "## Step 1: Hello, Data!\n",
    "Load primary_dataset_with_CouponCode.csv, display first 3 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "7dcdad22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Order Date",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Order ID",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Product",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Price",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Quantity",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Total Revenue",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Shipping City",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Coupon Code",
         "rawType": "object",
         "type": "unknown"
        }
       ],
       "ref": "3a7403d0-176b-4030-90fd-1d50e2e33288",
       "rows": [
        [
         "0",
         "10/18/2014",
         "686800706",
         "Cosmetics",
         "437.2",
         "8446",
         "3692591.2",
         "Medicine Hat",
         null
        ],
        [
         "1",
         "11/7/2011",
         "185941302",
         "Vegetables",
         "154.06",
         "3018",
         "464953.08",
         "Kingston",
         "P5XUQA0Z"
        ],
        [
         "2",
         "10/31/2016",
         "246222341",
         "Baby Food",
         "255.28",
         "1517",
         "387259.76",
         "Sudbury",
         null
        ]
       ],
       "shape": {
        "columns": 8,
        "rows": 3
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Order Date</th>\n",
       "      <th>Order ID</th>\n",
       "      <th>Product</th>\n",
       "      <th>Price</th>\n",
       "      <th>Quantity</th>\n",
       "      <th>Total Revenue</th>\n",
       "      <th>Shipping City</th>\n",
       "      <th>Coupon Code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10/18/2014</td>\n",
       "      <td>686800706</td>\n",
       "      <td>Cosmetics</td>\n",
       "      <td>437.20</td>\n",
       "      <td>8446</td>\n",
       "      <td>3692591.20</td>\n",
       "      <td>Medicine Hat</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11/7/2011</td>\n",
       "      <td>185941302</td>\n",
       "      <td>Vegetables</td>\n",
       "      <td>154.06</td>\n",
       "      <td>3018</td>\n",
       "      <td>464953.08</td>\n",
       "      <td>Kingston</td>\n",
       "      <td>P5XUQA0Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10/31/2016</td>\n",
       "      <td>246222341</td>\n",
       "      <td>Baby Food</td>\n",
       "      <td>255.28</td>\n",
       "      <td>1517</td>\n",
       "      <td>387259.76</td>\n",
       "      <td>Sudbury</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Order Date   Order ID     Product   Price  Quantity  Total Revenue  \\\n",
       "0  10/18/2014  686800706   Cosmetics  437.20      8446     3692591.20   \n",
       "1   11/7/2011  185941302  Vegetables  154.06      3018      464953.08   \n",
       "2  10/31/2016  246222341   Baby Food  255.28      1517      387259.76   \n",
       "\n",
       "  Shipping City Coupon Code  \n",
       "0  Medicine Hat         NaN  \n",
       "1      Kingston    P5XUQA0Z  \n",
       "2       Sudbury         NaN  "
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv('Original_Dataset/primary_dataset_with_CouponCode.csv')\n",
    "\n",
    "# Display first 3 rows\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec260a94",
   "metadata": {},
   "source": [
    "## Step 2: Pick the Right Container\n",
    "Depiction: Justify dict vs namedtuple vs class (1–2 sentences)\n",
    "\n",
    "Reason:\n",
    "I chose to use the class because the class can be flexible and easily expand a column, that convenient for future expansion and maintenance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa99f77d",
   "metadata": {},
   "source": [
    "## Step 3: Transaction Class and OO data structure\n",
    "Implement Transaction class and use it t populate an object-oriented data structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "603928de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transaction details:\n",
      "Transaction(Order_Date=2025-02-01, Order_ID=202510017, Product=Vegetables, Price=100, Quantity=2, Total_Revenue=200, Shipping_City=Kitchener, Coupon_Code=SAVE10PS)\n"
     ]
    }
   ],
   "source": [
    "# Define a Transaction class\n",
    "class Transaction:\n",
    "    def __init__(self, Order_Date, Order_ID, Product, Price, Quantity, Total_Revenue, Shipping_City, Coupon_Code):\n",
    "        self.Order_Date = Order_Date\n",
    "        self.Order_ID = Order_ID\n",
    "        self.Product = Product\n",
    "        self.Price = Price\n",
    "        self.Quantity = Quantity\n",
    "        self.Total_Revenue = Total_Revenue\n",
    "        self.Shipping_City = Shipping_City\n",
    "        self.Coupon_Code = Coupon_Code\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return (f\"Transaction(Order_Date={self.Order_Date}, Order_ID={self.Order_ID}, Product={self.Product}, Price={self.Price}, \"\n",
    "                f\"Quantity={self.Quantity}, Total_Revenue={self.Total_Revenue}, Shipping_City={self.Shipping_City}, Coupon_Code={self.Coupon_Code})\")\n",
    "\n",
    "\n",
    "# Create a list of Transaction objects\n",
    "tx = Transaction(\n",
    "    Order_Date = \"2025-02-01\",\n",
    "    Order_ID = \"202510017\",\n",
    "    Product = \"Vegetables\",\n",
    "    Price = 100,\n",
    "    Quantity = 2,\n",
    "    Total_Revenue = 200,\n",
    "    Shipping_City = \"Kitchener\",\n",
    "    Coupon_Code = \"SAVE10PS\"\n",
    ")\n",
    "\n",
    "\n",
    "# Display the transaction\n",
    "print(\"Transaction details:\")\n",
    "print(tx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec9cdf9",
   "metadata": {},
   "source": [
    "## Step 4: Bulk Loader\n",
    "Using the primary_dataset_with_CouponCode.csv to do load_transactions() returning list ↦ type-hinted "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "44bedfc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 transactions:\n",
      "Transaction(Order_Date=10/18/2014, Order_ID=686800706, Product=Cosmetics, Price=437.2, Quantity=8446, Total_Revenue=3692591.2, Shipping_City=Medicine Hat, Coupon_Code=nan)\n",
      "Transaction(Order_Date=11/7/2011, Order_ID=185941302, Product=Vegetables, Price=154.06, Quantity=3018, Total_Revenue=464953.08, Shipping_City=Kingston, Coupon_Code=P5XUQA0Z)\n",
      "Transaction(Order_Date=10/31/2016, Order_ID=246222341, Product=Baby Food, Price=255.28, Quantity=1517, Total_Revenue=387259.76, Shipping_City=Sudbury, Coupon_Code=nan)\n",
      "Transaction(Order_Date=4/10/2010, Order_ID=161442649, Product=Cereal, Price=205.7, Quantity=3322, Total_Revenue=683335.4, Shipping_City=Red Deer, Coupon_Code=nan)\n",
      "Transaction(Order_Date=8/16/2011, Order_ID=645713555, Product=Fruits, Price=9.33, Quantity=9845, Total_Revenue=91853.85, Shipping_City=Brandon, Coupon_Code=nan)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from typing import List\n",
    "\n",
    "# Define load transactions function\n",
    "def load_transactions(filename: str) -> List[Transaction]:\n",
    "    df = pd.read_csv(filename)\n",
    "    # create a list of Transaction objects\n",
    "    transactions = [\n",
    "        Transaction(\n",
    "            row['Order Date'],\n",
    "            row['Order ID'],\n",
    "            row['Product'],\n",
    "            float(row['Price']),\n",
    "            int(row['Quantity']),\n",
    "            float(row['Total Revenue']),\n",
    "            row['Shipping City'],\n",
    "            row['Coupon Code']\n",
    "        )\n",
    "        for _, row in df.iterrows()\n",
    "    ]\n",
    "    return transactions\n",
    "    \n",
    "\n",
    "# Use bulk loader, call the function to load transactions list\n",
    "transactions = load_transactions('Original_Dataset/primary_dataset_with_CouponCode.csv')\n",
    "\n",
    "# Display the first 5 transactions\n",
    "print(\"First 5 transactions:\")\n",
    "\n",
    "for tx in transactions[:5]:\n",
    "    print(tx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c47676",
   "metadata": {},
   "source": [
    "## Step 5: Quick Profiling\n",
    "(1) Calculate the Min/mean/max price \n",
    "(2) Display unique city count and cities list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "a3c1074b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min Price: 9.33\n",
      "Max Price: 668.27\n",
      "Mean Price: 274.29506\n",
      "unique cities count: 49\n",
      "cities list: {'Wood Buffalo', 'Victoria', 'Quebec City', 'Calgary', 'Medicine Hat', 'Peterborough', 'Kamloops', 'Belleville', 'Abbotsford', 'Saint-Jean-sur-Richelieu', 'Regina', 'Vernon', 'Edmonton', 'Drummondville', 'Sarnia', 'Chatham-Kent', 'Oshawa', 'Prince George', 'London', 'Moncton', 'Winnipeg', 'Fredericton', 'Kingston', 'Windsor', 'St. Catharines', 'Kelowna', 'Barrie', 'Nanaimo', 'Red Deer', 'Vancouver', 'Montreal', 'Kitchener', 'Lethbridge', 'Toronto', 'Saskatoon', 'Saint John', 'Sherbrooke', 'Thunder Bay', 'Brantford', 'Sudbury', 'Chilliwack', 'Halifax', 'Grande Prairie', 'Brandon', 'New Westminster', 'Hamilton', 'Ottawa', 'Moose Jaw', 'Guelph'}\n"
     ]
    }
   ],
   "source": [
    "# Get all list of Unit Prices\n",
    "prices = [tx.Price for tx in transactions]\n",
    "\n",
    "# Calculate min, mean, and max of Unit Prices\n",
    "min_price = min(prices)\n",
    "max_price = max(prices)\n",
    "mean_price = sum(prices) / len(prices)\n",
    "\n",
    "# Display the min, mean, and max of Unit Prices\n",
    "print(f\"Min Price: {min_price}\")\n",
    "print(f\"Max Price: {max_price}\")\n",
    "print(f\"Mean Price: {mean_price}\")\n",
    "\n",
    "# Get all list of Countrys(cities)\n",
    "cities = [tx.Shipping_City for tx in transactions]\n",
    "unique_cities = set(cities)\n",
    "print(f\"unique cities count: {len(unique_cities)}\")\n",
    "print(f\"cities list: {unique_cities}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d24be8c1",
   "metadata": {},
   "source": [
    "## Step 6: Spot the Grime \n",
    "Identify at least three dirty data cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "f3a51596",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "missing value of Coupon Code column: Transaction(Order_Date=10/18/2014, Order_ID=686800706, Product=Cosmetics, Price=437.2, Quantity=8446, Total_Revenue=3692591.2, Shipping_City=Medicine Hat, Coupon_Code=nan)\n",
      "missing value of Coupon Code column: Transaction(Order_Date=10/31/2016, Order_ID=246222341, Product=Baby Food, Price=255.28, Quantity=1517, Total_Revenue=387259.76, Shipping_City=Sudbury, Coupon_Code=nan)\n",
      "missing value of Coupon Code column: Transaction(Order_Date=4/10/2010, Order_ID=161442649, Product=Cereal, Price=205.7, Quantity=3322, Total_Revenue=683335.4, Shipping_City=Red Deer, Coupon_Code=nan)\n",
      "missing value of Coupon Code column: Transaction(Order_Date=8/16/2011, Order_ID=645713555, Product=Fruits, Price=9.33, Quantity=9845, Total_Revenue=91853.85, Shipping_City=Brandon, Coupon_Code=nan)\n",
      "missing value of Coupon Code column: Transaction(Order_Date=11/24/2014, Order_ID=683458888, Product=Cereal, Price=205.7, Quantity=9528, Total_Revenue=1959909.6, Shipping_City=Drummondville, Coupon_Code=nan)\n",
      "missing value of Coupon Code column: Transaction(Order_Date=5/17/2012, Order_ID=208630645, Product=Clothes, Price=109.28, Quantity=7299, Total_Revenue=797634.72, Shipping_City=Kamloops, Coupon_Code=nan)\n",
      "\n",
      "All Product categories: {'meat', 'Snacks', 'Personal Care', 'cereal', 'Meat', 'Office Supplies', 'vegetables', 'personal Care', 'Cosmetics', 'Fruits', 'beverages', 'clothes', 'Cereal', 'Baby Food', 'office Supplies', 'snacks', 'Vegetables', 'Beverages', 'Household', 'Clothes', 'fruits'}\n",
      "\n",
      "\n",
      "No unreasonable values found in Units Sold or Unit Cost columns.\n"
     ]
    }
   ],
   "source": [
    "# Case 1: find missing values in the 'Coupon Code' column\n",
    "count = 0\n",
    "for tx in transactions:\n",
    "    if tx.Coupon_Code == '' or str(tx.Coupon_Code).lower() == 'nan':\n",
    "        print(\"missing value of Coupon Code column:\", tx)\n",
    "        count += 1\n",
    "        # only display 5 missing values\n",
    "        if count > 5:\n",
    "            break\n",
    "\n",
    "# Case 2: find inconsistent categories in 'Product' column, check inconsistent capitalization of words\n",
    "Product = set(tx.Product for tx in transactions)\n",
    "print(f\"\\nAll Product categories:\", Product)\n",
    "print(\"\\n\")\n",
    "\n",
    "# Case 3: find impossible Values in Price and Total Revenue columns, if values are 0 or negative number\n",
    "found = False\n",
    "for tx in transactions:\n",
    "    if tx.Price <= 0:\n",
    "        print(\"Unreasonable Price:\", tx)\n",
    "        found = True\n",
    "    if tx.Total_Revenue <= 0:\n",
    "        print(\"Unreasonable Total_Revenue:\", tx)\n",
    "        found = True\n",
    "\n",
    "if not found:\n",
    "    print(\"No unreasonable values found in Units Sold or Unit Cost columns.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e74e5e",
   "metadata": {},
   "source": [
    "## Step 7: Cleaning Rules\n",
    "Execute fixes inside clean() and show “before/after” counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "72d4f602",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Coupon Code before cleaning: 351, after cleaning: 0\n",
      "Negative Price before cleaning: 0, after cleaning: 0\n",
      "Negative Total Revenue before cleaning: 0, after cleaning: 0\n",
      "Inconsistent Product before cleaning: 9, after cleaning: 0\n"
     ]
    }
   ],
   "source": [
    "# Define a clean function\n",
    "def clean(transactions):\n",
    "    # The number of anomalies before statistical correction\n",
    "    before_missing_coupon_code = sum(\n",
    "        tx.Coupon_Code == '' or str(tx.Coupon_Code).lower() == 'nan'\n",
    "        for tx in transactions\n",
    "    )\n",
    "    before_negative_price = sum(\n",
    "        tx.Price <= 0 \n",
    "        for tx in transactions\n",
    "    )\n",
    "    before_negative_total_revenue = sum(\n",
    "        tx.Total_Revenue <= 0 \n",
    "        for tx in transactions\n",
    "    )\n",
    "    before_product = set(tx.Product for tx in transactions)\n",
    "    before_product_title = set(tx.Product.title() for tx in transactions)\n",
    "    before_inconsistent_product = len(before_product - before_product_title)\n",
    "\n",
    "\n",
    "    cleaned = []\n",
    "    for tx in transactions:\n",
    "        # Clean Coupon Code dataset\n",
    "        Coupon_Code = tx.Coupon_Code\n",
    "        if Coupon_Code == '' or str(Coupon_Code).lower() == 'nan':\n",
    "            Coupon_Code = 'NONE'\n",
    "        \n",
    "        # Clean Price dataset\n",
    "        Price = tx.Price if tx.Price > 0 else 0\n",
    "        \n",
    "        # Clean Total Revenue dataset\n",
    "        Total_Revenue = tx.Total_Revenue if tx.Total_Revenue > 0 else 0\n",
    "\n",
    "        # Clean Item Type dataset\n",
    "        Product = tx.Product.title() # \"vegetables\" -> \"Vegetables\"\n",
    "        \n",
    "        # Generate a new Transaction object with cleaned data\n",
    "        cleaned_tx = Transaction(\n",
    "            tx.Order_Date,\n",
    "            tx.Order_ID,\n",
    "            Product,\n",
    "            Price,\n",
    "            tx.Quantity,\n",
    "            Total_Revenue,\n",
    "            tx.Shipping_City,\n",
    "            Coupon_Code\n",
    "        )\n",
    "        cleaned.append(cleaned_tx)\n",
    "    \n",
    "    # The number of anomalies after statistical correction\n",
    "    after_missing_coupon_code = sum(\n",
    "        tx.Coupon_Code == '' or str(tx.Coupon_Code).lower() == 'nan'\n",
    "        for tx in cleaned\n",
    "    )\n",
    "    after_negative_price = sum(\n",
    "        tx.Price <= 0 \n",
    "        for tx in cleaned\n",
    "    )\n",
    "    after_negative_total_revenue = sum(\n",
    "        tx.Total_Revenue <= 0 \n",
    "        for tx in cleaned\n",
    "    )\n",
    "    after_product = set(tx.Product for tx in cleaned)\n",
    "    after_product_title = set(tx.Product.title() for tx in cleaned)\n",
    "    after_inconsistent_product = len(after_product - after_product_title)\n",
    "\n",
    "    # Print the number of anomalies before and after cleaning\n",
    "    print(f\"Missing Coupon Code before cleaning: {before_missing_coupon_code}, after cleaning: {after_missing_coupon_code}\")\n",
    "    print(f\"Negative Price before cleaning: {before_negative_price}, after cleaning: {after_negative_price}\")\n",
    "    print(f\"Negative Total Revenue before cleaning: {before_negative_total_revenue}, after cleaning: {after_negative_total_revenue}\")\n",
    "    print(f\"Inconsistent Product before cleaning: {before_inconsistent_product}, after cleaning: {after_inconsistent_product}\")\n",
    "    \n",
    "    return cleaned\n",
    "\n",
    "# Perform cleanup and display the number of exceptions before and after\n",
    "cleaned_transactions = clean(transactions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c1a09a",
   "metadata": {},
   "source": [
    "## Step 8: Transformations\n",
    "Parse coupon_code ➞ numeric discount (others apply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "2b1f0de2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Order_Date': '10/18/2014', 'Order_ID': 686800706, 'Product': 'Cosmetics', 'Discounted_Price': 437.2, 'Quantity': 8446, 'Discounted_Total_Revenue': 3692591.2, 'Shipping_City': 'Medicine Hat', 'Coupon_Code': 'NONE', 'Discount': 0}\n",
      "{'Order_Date': '11/7/2011', 'Order_ID': 185941302, 'Product': 'Vegetables', 'Discounted_Price': 146.357, 'Quantity': 3018, 'Discounted_Total_Revenue': 441705.426, 'Shipping_City': 'Kingston', 'Coupon_Code': 'P5XUQA0Z', 'Discount': 0.05}\n",
      "{'Order_Date': '10/31/2016', 'Order_ID': 246222341, 'Product': 'Baby Food', 'Discounted_Price': 255.28, 'Quantity': 1517, 'Discounted_Total_Revenue': 387259.76, 'Shipping_City': 'Sudbury', 'Coupon_Code': 'NONE', 'Discount': 0}\n",
      "{'Order_Date': '4/10/2010', 'Order_ID': 161442649, 'Product': 'Cereal', 'Discounted_Price': 205.7, 'Quantity': 3322, 'Discounted_Total_Revenue': 683335.4, 'Shipping_City': 'Red Deer', 'Coupon_Code': 'NONE', 'Discount': 0}\n",
      "{'Order_Date': '8/16/2011', 'Order_ID': 645713555, 'Product': 'Fruits', 'Discounted_Price': 9.33, 'Quantity': 9845, 'Discounted_Total_Revenue': 91853.85, 'Shipping_City': 'Brandon', 'Coupon_Code': 'NONE', 'Discount': 0}\n"
     ]
    }
   ],
   "source": [
    "# Coupon Code transformation to discount item\n",
    "def transform_with_discount(transactions):\n",
    "    coupon_to_discount = {\n",
    "        'NONE': 0,\n",
    "        '1DIHM8JO': 0.1,\n",
    "        '22EYACAF': 0.2,\n",
    "        '40CXXUES': 0.3,\n",
    "    }\n",
    "\n",
    "    transformed = []\n",
    "    for tx in transactions:\n",
    "        code = tx.Coupon_Code\n",
    "        # Default discount 5%\n",
    "        discount = coupon_to_discount.get(code, 0.05)\n",
    "\n",
    "        tx_dict = {\n",
    "            'Order_Date': tx.Order_Date,\n",
    "            'Order_ID': tx.Order_ID,\n",
    "            'Product': tx.Product,\n",
    "            'Discounted_Price': tx.Price * (1 - discount),\n",
    "            'Quantity': tx.Quantity,\n",
    "            'Discounted_Total_Revenue': tx.Total_Revenue * (1 - discount),\n",
    "            'Shipping_City': tx.Shipping_City,\n",
    "            'Coupon_Code': code,\n",
    "            'Discount': discount\n",
    "        }\n",
    "        transformed.append(tx_dict)\n",
    "\n",
    "    return transformed\n",
    "\n",
    "# Run the transformation\n",
    "transformed_transactions = transform_with_discount(cleaned_transactions)\n",
    "\n",
    "# Display the first 5 transformed transactions\n",
    "for tx in transformed_transactions[:5]:\n",
    "    print(tx)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d48081eb",
   "metadata": {},
   "source": [
    "## Step 9: Feature Engineering\n",
    "Add days_since_purchase feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "0c7b7ae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Order_Date': '10/18/2014', 'Order_ID': 686800706, 'Product': 'Cosmetics', 'Discounted_Price': 437.2, 'Quantity': 8446, 'Discounted_Total_Revenue': 3692591.2, 'Shipping_City': 'Medicine Hat', 'Coupon_Code': 'NONE', 'Discount': 0, 'Days_Since_Purchase': 3874, 'Order_Month': 10, 'Order_Season': 'Fall'}\n",
      "{'Order_Date': '11/7/2011', 'Order_ID': 185941302, 'Product': 'Vegetables', 'Discounted_Price': 146.357, 'Quantity': 3018, 'Discounted_Total_Revenue': 441705.426, 'Shipping_City': 'Kingston', 'Coupon_Code': 'P5XUQA0Z', 'Discount': 0.05, 'Days_Since_Purchase': 4950, 'Order_Month': 11, 'Order_Season': 'Fall'}\n",
      "{'Order_Date': '10/31/2016', 'Order_ID': 246222341, 'Product': 'Baby Food', 'Discounted_Price': 255.28, 'Quantity': 1517, 'Discounted_Total_Revenue': 387259.76, 'Shipping_City': 'Sudbury', 'Coupon_Code': 'NONE', 'Discount': 0, 'Days_Since_Purchase': 3130, 'Order_Month': 10, 'Order_Season': 'Fall'}\n",
      "{'Order_Date': '4/10/2010', 'Order_ID': 161442649, 'Product': 'Cereal', 'Discounted_Price': 205.7, 'Quantity': 3322, 'Discounted_Total_Revenue': 683335.4, 'Shipping_City': 'Red Deer', 'Coupon_Code': 'NONE', 'Discount': 0, 'Days_Since_Purchase': 5526, 'Order_Month': 4, 'Order_Season': 'Spring'}\n",
      "{'Order_Date': '8/16/2011', 'Order_ID': 645713555, 'Product': 'Fruits', 'Discounted_Price': 9.33, 'Quantity': 9845, 'Discounted_Total_Revenue': 91853.85, 'Shipping_City': 'Brandon', 'Coupon_Code': 'NONE', 'Discount': 0, 'Days_Since_Purchase': 5033, 'Order_Month': 8, 'Order_Season': 'Summer'}\n"
     ]
    }
   ],
   "source": [
    "# Importing the datetime module\n",
    "from datetime import datetime\n",
    "\n",
    "# Adding Days_Since_Purchase feature\n",
    "def transform_with_features(transactions):\n",
    "    transformed = []\n",
    "    today = datetime.today()\n",
    "    for tx in transactions:\n",
    "        order_date = datetime.strptime(tx['Order_Date'], \"%m/%d/%Y\")\n",
    "        # Calculate the number of days between Order_Date and today\n",
    "        days_since_purchase = (today - order_date).days\n",
    "\n",
    "        # Order month\n",
    "        order_month = order_date.month\n",
    "        # Order season\n",
    "        if order_month in [3, 4, 5]:\n",
    "            order_season = 'Spring'\n",
    "        elif order_month in [6, 7, 8]:\n",
    "            order_season = 'Summer'\n",
    "        elif order_month in [9, 10, 11]:\n",
    "            order_season = 'Fall'\n",
    "        else:\n",
    "            order_season = 'Winter'\n",
    "\n",
    "        # Create a dictionary with the transformed data\n",
    "        tx_dict = dict(tx)\n",
    "        tx_dict['Days_Since_Purchase'] = days_since_purchase\n",
    "        tx_dict['Order_Month'] = order_month\n",
    "        tx_dict['Order_Season'] = order_season\n",
    "\n",
    "        transformed.append(tx_dict)\n",
    "\n",
    "    return transformed\n",
    "\n",
    "# Run the transformation\n",
    "feature_transactions = transform_with_features(transformed_transactions)\n",
    "\n",
    "# Display the first 5 transformed transactions\n",
    "for tx in feature_transactions[:5]:\n",
    "    print(tx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b351856",
   "metadata": {},
   "source": [
    "## Step 10:  Mini-Aggregation\n",
    "Using dict or pandas.groupby to do revenue per shipping_city"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa4613d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistics Revenue per Shipping City:\n",
      "Abbotsford: 20645214.87\n",
      "Barrie: 9119996.10\n",
      "Belleville: 16526917.54\n",
      "Brandon: 17300731.03\n",
      "Brantford: 14046608.79\n",
      "Calgary: 26098957.19\n",
      "Chatham-Kent: 13150893.38\n",
      "Chilliwack: 11501475.66\n",
      "Drummondville: 26589884.68\n",
      "Edmonton: 5243855.35\n",
      "Fredericton: 4591646.99\n",
      "Grande Prairie: 8699784.46\n",
      "Guelph: 6358982.79\n",
      "Halifax: 17553729.49\n",
      "Hamilton: 12256136.38\n",
      "Kamloops: 19138942.45\n",
      "Kelowna: 14530392.29\n",
      "Kingston: 14025164.93\n",
      "Kitchener: 16791994.67\n",
      "Lethbridge: 17684923.27\n",
      "London: 5880386.29\n",
      "Medicine Hat: 16621153.60\n",
      "Moncton: 35640351.90\n",
      "Montreal: 4603649.23\n",
      "Moose Jaw: 13497338.30\n",
      "Nanaimo: 7265879.06\n",
      "New Westminster: 10418812.08\n",
      "Oshawa: 22561448.52\n",
      "Ottawa: 5030918.39\n",
      "Peterborough: 8791063.25\n",
      "Prince George: 21084309.83\n",
      "Quebec City: 18702403.12\n",
      "Red Deer: 13072665.21\n",
      "Regina: 10835347.04\n",
      "Saint John: 6878318.27\n",
      "Saint-Jean-sur-Richelieu: 25439820.67\n",
      "Sarnia: 15579157.63\n",
      "Saskatoon: 7078354.75\n",
      "Sherbrooke: 14336089.06\n",
      "St. Catharines: 14133550.92\n",
      "Sudbury: 18095116.00\n",
      "Thunder Bay: 13897412.35\n",
      "Toronto: 16508013.48\n",
      "Vancouver: 15726810.32\n",
      "Vernon: 9863765.60\n",
      "Victoria: 18429005.16\n",
      "Windsor: 18590821.56\n",
      "Winnipeg: 7756711.20\n",
      "Wood Buffalo: 13315334.46\n"
     ]
    }
   ],
   "source": [
    "# Statistics revenue per Shipping City using python dict\n",
    "def calculate_revenue_per_cit(transactions):\n",
    "    revenue_by_city = {}\n",
    "    for tx in transactions:\n",
    "        city = tx['Shipping_City']  # Assuming 'Country' is the shipping city\n",
    "        revenue = tx['Discounted_Total_Revenue']\n",
    "        if city not in revenue_by_city:\n",
    "            revenue_by_city[city] = 0\n",
    "        revenue_by_city[city] += revenue\n",
    "    \n",
    "    return revenue_by_city\n",
    "\n",
    "# Calculate revenue per city\n",
    "revenue_per_city = calculate_revenue_per_cit(feature_transactions)\n",
    "\n",
    "# Display the revenue per city\n",
    "print(\"Statistics Revenue per Shipping City:\")\n",
    "for city, revenue in sorted(revenue_per_city.items()):\n",
    "    print(f\"{city}: {revenue:.2f}\")\n",
    "\n",
    "# Save revenue per Shipping City to a CSV file\n",
    "#import pandas as pd\n",
    "\n",
    "#df = pd.DataFrame(list(revenue_per_city.items()), columns=['City', 'Total Revenue'])\n",
    "#df = df.sort_values(by='Total Revenue', ascending=False) # Sort by Total Revenue from highest to lowest\n",
    "#df.to_csv('Organize_Dataset/total_revenue_by_city.csv', index=False, encoding='utf-8', float_format='%.2f')\n",
    "#print(\"Total Revenue by city saved to 'total_revenue_by_city.csv'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c2fac4",
   "metadata": {},
   "source": [
    "## Step 11: Serialization Checkpoint\n",
    "Save cleaned data(primary_dataset_with_CouponCode.csv) to JSON and Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "77b7db2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serialization checkpoint complete: CSV, JSON and Parquet saved.\n",
      "   Order_Date   Order_ID     Product  Discounted_Price  Quantity  \\\n",
      "0  10/18/2014  686800706   Cosmetics            437.20      8446   \n",
      "1   11/7/2011  185941302  Vegetables            146.36      3018   \n",
      "2  10/31/2016  246222341   Baby Food            255.28      1517   \n",
      "\n",
      "   Discounted_Total_Revenue Shipping_City Coupon_Code  Discount  \\\n",
      "0                3692591.20  Medicine Hat        NONE      0.00   \n",
      "1                 441705.43      Kingston    P5XUQA0Z      0.05   \n",
      "2                 387259.76       Sudbury        NONE      0.00   \n",
      "\n",
      "   Days_Since_Purchase  Order_Month Order_Season  \n",
      "0                 3874           10         Fall  \n",
      "1                 4950           11         Fall  \n",
      "2                 3130           10         Fall  \n",
      "\n",
      "\n",
      "   Order_Date   Order_ID     Product  Discounted_Price  Quantity  \\\n",
      "0  10/18/2014  686800706   Cosmetics            437.20      8446   \n",
      "1   11/7/2011  185941302  Vegetables            146.36      3018   \n",
      "2  10/31/2016  246222341   Baby Food            255.28      1517   \n",
      "\n",
      "   Discounted_Total_Revenue Shipping_City Coupon_Code  Discount  \\\n",
      "0                3692591.20  Medicine Hat        NONE      0.00   \n",
      "1                 441705.43      Kingston    P5XUQA0Z      0.05   \n",
      "2                 387259.76       Sudbury        NONE      0.00   \n",
      "\n",
      "   Days_Since_Purchase  Order_Month Order_Season  \n",
      "0                 3874           10         Fall  \n",
      "1                 4950           11         Fall  \n",
      "2                 3130           10         Fall  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(feature_transactions)\n",
    "\n",
    "# Round only float fields \n",
    "float_cols = df.select_dtypes(include=['float']).columns\n",
    "df[float_cols] = df[float_cols].round(2)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv('Organize_Dataset/primary_dataset_with_CouponCode_Serialization.csv', index=False, encoding='utf-8')\n",
    "\n",
    "# Save the DataFrame to a JSON file\n",
    "df.to_json('Organize_Dataset/primary_dataset_with_CouponCode_Serialization.json', orient='records', force_ascii=False)\n",
    "\n",
    "# Save the DataFrame to a Parquet file\n",
    "df.to_parquet('Organize_Dataset/primary_dataset_with_CouponCode_Serialization.parquet', index=False)\n",
    "\n",
    "print(\"Serialization checkpoint complete: CSV, JSON and Parquet saved.\")\n",
    "\n",
    "# Load the JSON and Parquet files\n",
    "df_jaon = pd.read_json('Organize_Dataset/primary_dataset_with_CouponCode_Serialization.json')\n",
    "print(df_jaon.head(3))\n",
    "print(\"\\n\")\n",
    "\n",
    "df_parquet = pd.read_parquet('Organize_Dataset/primary_dataset_with_CouponCode_Serialization.parquet')\n",
    "print(df_parquet.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e287de",
   "metadata": {},
   "source": [
    "## Step 12: Soft Interview Reflection\n",
    "Explaining how OOP helped (< 120 words)\n",
    "\n",
    "Reason: When I use Object-Oriented Programming (OOP) to write my code, I can easily clean data, transform data items, and perform data feature engineering. This can display the dataset status clearly, making it easy to identify the data information and perform further data analysis. Also, OOP can reuse code efficiently and quickly maintain my code. Thus, I can focus on some functional requirements to modify the code at any time. On the other hand, OOP can improve code readability and extensibility. This helps me organize the data and analyze the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f07e0cdb",
   "metadata": {},
   "source": [
    "## Data-Dictionary\n",
    "(1) Merge field definitions from the primary_dataset_with_CouponCode.csv header and the secondary_dataset.csv metadata source.\n",
    "(2) Present as a tidy Markdown table including the new columns, for example: Field, Type, Description, Source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "15498818",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Field'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/5f/yqkj_zgn7qzcqxxmhxl7rk9c0000gn/T/ipykernel_26334/1716200531.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0msecondary_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msecondary_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'Column'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'Field'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0msecondary_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Source'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Secondary'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# Merge primary and secondary fields\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mmerged\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprimary_fields\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msecondary_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Field'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'left'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msuffixes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_sec'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mmerged\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Type'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmerged\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Type'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Unknown'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[1;32m  10828\u001b[0m         \u001b[0mvalidate\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mMergeValidate\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  10829\u001b[0m     \u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  10830\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmerge\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  10831\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m> 10832\u001b[0;31m         return merge(\n\u001b[0m\u001b[1;32m  10833\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  10834\u001b[0m             \u001b[0mright\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  10835\u001b[0m             \u001b[0mhow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhow\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[1;32m    166\u001b[0m             \u001b[0mvalidate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m             \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m         op = _MergeOperation(\n\u001b[0m\u001b[1;32m    171\u001b[0m             \u001b[0mleft_df\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m             \u001b[0mright_df\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m             \u001b[0mhow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhow\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, indicator, validate)\u001b[0m\n\u001b[1;32m    790\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mright_join_keys\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    792\u001b[0m             \u001b[0mleft_drop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    793\u001b[0m             \u001b[0mright_drop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 794\u001b[0;31m         \u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_merge_keys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    795\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    796\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mleft_drop\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    797\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleft\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_drop_labels_or_levels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft_drop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1293\u001b[0m                         \u001b[0;31m# Then we're either Hashable or a wrong-length arraylike,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1294\u001b[0m                         \u001b[0;31m#  the latter of which will raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1295\u001b[0m                         \u001b[0mrk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mHashable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1296\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mrk\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1297\u001b[0;31m                             \u001b[0mright_keys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mright\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_label_or_level_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1298\u001b[0m                         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1299\u001b[0m                             \u001b[0;31m# work-around for merge_asof(right_index=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1300\u001b[0m                             \u001b[0mright_keys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mright\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1907\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mother_axes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1908\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_level_reference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1909\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_level_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1910\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1911\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1912\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1913\u001b[0m         \u001b[0;31m# Check for duplicates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1914\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Field'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the primary data fields\n",
    "primary_df = pd.read_csv('Organize_Dataset/primary_dataset_with_CouponCode_Serialization.csv', nrows=0)\n",
    "primary_fields = pd.DataFrame({'Field': primary_df.columns})\n",
    "primary_fields['Source'] = 'Primary'\n",
    "                              \n",
    "# Read secondary metadata\n",
    "secondary_df = pd.read_csv('Original_Dataset/secondary_dataset.csv')\n",
    "# If the field name is not 'Field', please adjust\n",
    "if 'Field' not in secondary_df.columns:\n",
    "    # If is 'Column'\n",
    "    secondary_df = secondary_df.rename(columns={'Column': 'Field'})\n",
    "secondary_df['Source'] = 'Secondary'\n",
    "\n",
    "# Merge primary and secondary fields\n",
    "merged = primary_fields.merge(secondary_df, on='Field', how='left', suffixes=('', '_sec'))\n",
    "\n",
    "#\n",
    "merged['Type'] = merged['Type'].fillna('Unknown')\n",
    "merged['Description'] = merged['Description'].fillna('No description')\n",
    "merged['Source'] = merged['Source'].fillna('Primary')\n",
    "\n",
    "# Output Markdown table\n",
    "merged = merged[['Field', 'Type', 'Description', 'Source']]\n",
    "\n",
    "print(\"| Field | Type | Description | Source |\")\n",
    "print(\"|-------|------|-------------|--------|\")\n",
    "for _, row in merged.iterrows():\n",
    "    print(f\"| {row['Field']} | {row['Type']} | {row['Description']} | {row['Source']} |\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
